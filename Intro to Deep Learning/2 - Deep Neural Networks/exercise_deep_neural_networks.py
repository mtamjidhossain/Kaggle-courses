{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# **This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/deep-neural-networks).**\n# \n# ---\n# \n\n# %% [markdown]\n# # Introduction #\n# \n# In the tutorial, we saw how to build deep neural networks by stacking layers inside a `Sequential` model. By adding an *activation function* after the hidden layers, we gave the network the ability to learn more complex (non-linear) relationships in the data.\n# \n# In these exercises, you'll build a neural network with several hidden layers and then explore some activation functions beyond ReLU. Run this next cell to set everything up!\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:42:56.694025Z\",\"iopub.execute_input\":\"2021-11-05T21:42:56.694415Z\",\"iopub.status.idle\":\"2021-11-05T21:42:56.699648Z\",\"shell.execute_reply.started\":\"2021-11-05T21:42:56.694386Z\",\"shell.execute_reply\":\"2021-11-05T21:42:56.698957Z\"}}\nimport tensorflow as tf\n\n# Setup plotting\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex2 import *\n\n# %% [markdown]\n# In the *Concrete* dataset, your task is to predict the compressive strength of concrete manufactured according to various recipes.\n# \n# Run the next code cell without changes to load the dataset.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:43:48.572279Z\",\"iopub.execute_input\":\"2021-11-05T21:43:48.573126Z\",\"iopub.status.idle\":\"2021-11-05T21:43:48.609563Z\",\"shell.execute_reply.started\":\"2021-11-05T21:43:48.573087Z\",\"shell.execute_reply\":\"2021-11-05T21:43:48.608790Z\"}}\nimport pandas as pd\n\nconcrete = pd.read_csv('../input/dl-course-data/concrete.csv')\nconcrete.head()\n\n# %% [markdown]\n# # 1) Input Shape #\n# \n# The target for this task is the column `'CompressiveStrength'`. The remaining columns are the features we'll use as inputs.\n# \n# What would be the input shape for this dataset?\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:44:45.788876Z\",\"iopub.execute_input\":\"2021-11-05T21:44:45.789457Z\",\"iopub.status.idle\":\"2021-11-05T21:44:45.795076Z\",\"shell.execute_reply.started\":\"2021-11-05T21:44:45.789412Z\",\"shell.execute_reply\":\"2021-11-05T21:44:45.794183Z\"}}\nconcrete.shape\n\n# %% [code] {\"lines_to_next_cell\":2,\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:45:15.741647Z\",\"iopub.execute_input\":\"2021-11-05T21:45:15.742509Z\",\"iopub.status.idle\":\"2021-11-05T21:45:15.750308Z\",\"shell.execute_reply.started\":\"2021-11-05T21:45:15.742452Z\",\"shell.execute_reply\":\"2021-11-05T21:45:15.749466Z\"}}\n# YOUR CODE HERE\ninput_shape = [8]\n\n# Check your answer\nq_1.check()\n\n# %% [code]\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n# %% [markdown]\n# # 2) Define a Model with Hidden Layers #\n# \n# Now create a model with three hidden layers, each having 512 units and the ReLU activation.  Be sure to include an output layer of one unit and no activation, and also `input_shape` as an argument to the first layer.\n\n# %% [code] {\"lines_to_next_cell\":0,\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:49:19.039902Z\",\"iopub.execute_input\":\"2021-11-05T21:49:19.040184Z\",\"iopub.status.idle\":\"2021-11-05T21:49:19.090848Z\",\"shell.execute_reply.started\":\"2021-11-05T21:49:19.040154Z\",\"shell.execute_reply\":\"2021-11-05T21:49:19.090065Z\"}}\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# YOUR CODE HERE\nmodel = keras.Sequential([\n    layers.Dense(units= 512, activation= 'relu',input_shape= [8]),\n    layers.Dense(units= 512, activation= 'relu'),\n    layers.Dense(units= 512, activation= 'relu'),\n    layers.Dense(units= 1)\n])\n\n# Check your answer\nq_2.check()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:49:42.066487Z\",\"iopub.execute_input\":\"2021-11-05T21:49:42.067254Z\",\"iopub.status.idle\":\"2021-11-05T21:49:42.071518Z\",\"shell.execute_reply.started\":\"2021-11-05T21:49:42.067213Z\",\"shell.execute_reply\":\"2021-11-05T21:49:42.070634Z\"}}\n# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()\n\n# %% [markdown]\n# # 3) Activation Layers #\n# \n# Let's explore activations functions some.\n# \n# The usual way of attaching an activation function to a `Dense` layer is to include it as part of the definition with the `activation` argument. Sometimes though you'll want to put some other layer between the `Dense` layer and its activation function. (We'll see an example of this in Lesson 5 with *batch normalization*.) In this case, we can define the activation in its own `Activation` layer, like so:\n# \n# ```\n# layers.Dense(units=8),\n# layers.Activation('relu')\n# ```\n# \n# This is completely equivalent to the ordinary way: `layers.Dense(units=8, activation='relu')`.\n# \n# Rewrite the following model so that each activation is in its own `Activation` layer.\n\n# %% [code]\n### YOUR CODE HERE: rewrite this to use activation layers\nmodel = keras.Sequential([\n    layers.Dense(32, activation='relu', input_shape=[8]),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1),\n])\n\n# %% [code] {\"lines_to_next_cell\":0,\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:54:34.243587Z\",\"iopub.execute_input\":\"2021-11-05T21:54:34.244393Z\",\"iopub.status.idle\":\"2021-11-05T21:54:34.286685Z\",\"shell.execute_reply.started\":\"2021-11-05T21:54:34.244349Z\",\"shell.execute_reply\":\"2021-11-05T21:54:34.285844Z\"}}\nmodel = keras.Sequential([\n    layers.Dense(units= 32, input_shape=[8]),\n    layers.Activation('relu'),\n    layers.Dense(units= 32),\n    layers.Activation('relu'),\n    layers.Dense(units= 1),\n])\n\n# Check your answer\nq_3.check()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:54:50.807250Z\",\"iopub.execute_input\":\"2021-11-05T21:54:50.807631Z\",\"iopub.status.idle\":\"2021-11-05T21:54:50.810959Z\",\"shell.execute_reply.started\":\"2021-11-05T21:54:50.807584Z\",\"shell.execute_reply\":\"2021-11-05T21:54:50.810255Z\"}}\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n# %% [markdown]\n# # Optional: Alternatives to ReLU #\n# \n# There is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with.\n# \n# Let's look at the graphs of some of these. Change the activation from `'relu'` to one of the others named above. Then run the cell to see the graph. (Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for more ideas.)\n\n# %% [code] {\"lines_to_next_cell\":0,\"execution\":{\"iopub.status.busy\":\"2021-11-05T21:57:27.775846Z\",\"iopub.execute_input\":\"2021-11-05T21:57:27.776109Z\",\"iopub.status.idle\":\"2021-11-05T21:57:28.014389Z\",\"shell.execute_reply.started\":\"2021-11-05T21:57:27.776082Z\",\"shell.execute_reply\":\"2021-11-05T21:57:28.013437Z\"}}\n# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else\nactivation_layer = layers.Activation('swish')\n\nx = tf.linspace(-3.0, 3.0, 100)\ny = activation_layer(x) # once created, a layer is callable just like a function\n\nplt.figure(dpi=100)\nplt.plot(x, y)\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.show()\n\n# %% [markdown]\n# # Keep Going #\n# \n# Now move on to Lesson 3 and [**learn how to train neural networks**](https://www.kaggle.com/ryanholbrook/stochastic-gradient-descent) with stochastic gradient descent.","metadata":{"_uuid":"b0235db3-71a4-434f-9192-8566bf0d2f52","_cell_guid":"445c87df-0c3a-4e0c-a26d-2951aaadd6cf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-05T21:59:23.643513Z","iopub.execute_input":"2021-11-05T21:59:23.643839Z","iopub.status.idle":"2021-11-05T21:59:24.005108Z","shell.execute_reply.started":"2021-11-05T21:59:23.643799Z","shell.execute_reply":"2021-11-05T21:59:24.004298Z"},"trusted":true},"execution_count":27,"outputs":[]}]}